{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Dataset-V3_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\envs\\python\\lib\\site-packages\\pandas\\core\\frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "data_dup = data[['Team2', 'Team1', 'Score2', 'Score1', 'Date/Time', 'nb_matches2', 'nb_matches1', 'goals_past2', 'goals_past1',\n",
    "                 'ratio_goals_past2', 'ratio_goals_past1', 'nb_wins2', 'nb_wins1', 'ratio_wins2', 'ratio_wins1', 'nb_losses2',\n",
    "                 'nb_losses1', 'ratio_losses2', 'ratio_losses1', 'nb_draws2', 'nb_draws1', 'ratio_draws2', 'ratio_draws1',\n",
    "                 'nb_wins_opp2', 'nb_wins_opp1', 'ratio_wins_opp2', 'ratio_wins_opp1', 'nb_losses_opp2', 'nb_losses_opp1',\n",
    "                 'ratio_losses_opp2', 'ratio_losses_opp1', 'nb_draws_opp2', 'nb_draws_opp1', 'ratio_draws_opp2',\n",
    "                 'ratio_draws_opp1']]\n",
    "data_dup.columns = ['Team1', 'Team2', 'Score1', 'Score2', 'Date/Time', 'nb_matches1', 'nb_matches2', 'goals_past1', \n",
    "                    'goals_past2', 'ratio_goals_past1', 'ratio_goals_past2', 'nb_wins1', 'nb_wins2', 'ratio_wins1',\n",
    "                    'ratio_wins2', 'nb_losses1', 'nb_losses2', 'ratio_losses1', 'ratio_losses2', 'nb_draws1', 'nb_draws2', \n",
    "                    'ratio_draws1', 'ratio_draws2', 'nb_wins_opp1', 'nb_wins_opp2', 'ratio_wins_opp1',\n",
    "                    'ratio_wins_opp2', 'nb_losses_opp1', 'nb_losses_opp2', 'ratio_losses_opp1', 'ratio_losses_opp2', \n",
    "                    'nb_draws_opp1', 'nb_draws_opp2', 'ratio_draws_opp1', 'ratio_draws_opp2']\n",
    "data = data.append(data_dup, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = data.loc[data[\"Date/Time\"] >= '2018']\n",
    "df_train = data.loc[data[\"Date/Time\"] < '2018']\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Target Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "for i in range(len(df_train)):\n",
    "    if df_train['Score1'][i] > df_train['Score2'][i]:\n",
    "        y_train.append(1)\n",
    "    elif df_train['Score1'][i] < df_train['Score2'][i]:\n",
    "        y_train.append(2)\n",
    "    elif df_train['Score1'][i] == df_train['Score2'][i]:\n",
    "        y_train.append(0)\n",
    "        \n",
    "y_test = []\n",
    "for i in range(len(df_test)):\n",
    "    if df_test['Score1'][i] > df_test['Score2'][i]:\n",
    "        y_test.append(1)\n",
    "    elif df_test['Score1'][i] < df_test['Score2'][i]:\n",
    "        y_test.append(2)\n",
    "    elif df_test['Score1'][i] == df_test['Score2'][i]:\n",
    "        y_test.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting Useless Columns: Getting X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(['Team1','Team2','Score1','Score2','Date/Time','date_minus_1year'],axis=1)\n",
    "X_test = df_test.drop(['Team1','Team2','Score1','Score2','Date/Time','date_minus_1year'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date/Time', 'Score1', 'Score2', 'Team1', 'Team2', 'date_minus_1year',\n",
       "       'goals_past1', 'goals_past2', 'nb_draws1', 'nb_draws2', 'nb_draws_opp1',\n",
       "       'nb_draws_opp2', 'nb_losses1', 'nb_losses2', 'nb_losses_opp1',\n",
       "       'nb_losses_opp2', 'nb_matches1', 'nb_matches2', 'nb_wins1', 'nb_wins2',\n",
       "       'nb_wins_opp1', 'nb_wins_opp2', 'ratio_draws1', 'ratio_draws2',\n",
       "       'ratio_draws_opp1', 'ratio_draws_opp2', 'ratio_goals_past1',\n",
       "       'ratio_goals_past2', 'ratio_losses1', 'ratio_losses2',\n",
       "       'ratio_losses_opp1', 'ratio_losses_opp2', 'ratio_wins1', 'ratio_wins2',\n",
       "       'ratio_wins_opp1', 'ratio_wins_opp2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\envs\\python\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\ASUS\\Anaconda3\\envs\\python\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\ASUS\\Anaconda3\\envs\\python\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out Classification Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3627450980392157"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DummyClassifier(strategy = 'most_frequent')\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        84\n",
      "           1       0.36      1.00      0.53       111\n",
      "           2       0.00      0.00      0.00       111\n",
      "\n",
      "   micro avg       0.36      0.36      0.36       306\n",
      "   macro avg       0.12      0.33      0.18       306\n",
      "weighted avg       0.13      0.36      0.19       306\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\envs\\python\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,  84,   0],\n",
       "       [  0, 111,   0],\n",
       "       [  0, 111,   0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3660130718954248"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.04      0.06        84\n",
      "           1       0.39      0.48      0.43       111\n",
      "           2       0.39      0.50      0.44       111\n",
      "\n",
      "   micro avg       0.37      0.37      0.37       306\n",
      "   macro avg       0.30      0.34      0.31       306\n",
      "weighted avg       0.31      0.37      0.33       306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goals_past1  -->  0.07489886897669522\n",
      "goals_past2  -->  0.0749278150663895\n",
      "nb_draws1  -->  0.030078971702805567\n",
      "nb_draws2  -->  0.029942413437724528\n",
      "nb_draws_opp1  -->  0.006314426156482865\n",
      "nb_draws_opp2  -->  0.006291258310352071\n",
      "nb_losses1  -->  0.026168482617474828\n",
      "nb_losses2  -->  0.026253083007849426\n",
      "nb_losses_opp1  -->  0.004930178746751074\n",
      "nb_losses_opp2  -->  0.004969659764077542\n",
      "nb_matches1  -->  0.048630421855950005\n",
      "nb_matches2  -->  0.04858195374334901\n",
      "nb_wins1  -->  0.026329519922570464\n",
      "nb_wins2  -->  0.026078642929554526\n",
      "nb_wins_opp1  -->  0.004935102348324171\n",
      "nb_wins_opp2  -->  0.004967914245541911\n",
      "ratio_draws1  -->  0.05028597559854363\n",
      "ratio_draws2  -->  0.05014971200398447\n",
      "ratio_draws_opp1  -->  0.0068291096123399946\n",
      "ratio_draws_opp2  -->  0.0068386706631061245\n",
      "ratio_goals_past1  -->  0.10097324089768177\n",
      "ratio_goals_past2  -->  0.10072578953630712\n",
      "ratio_losses1  -->  0.05736488555821059\n",
      "ratio_losses2  -->  0.057332032805532326\n",
      "ratio_losses_opp1  -->  0.005799650121658088\n",
      "ratio_losses_opp2  -->  0.00580668344210471\n",
      "ratio_wins1  -->  0.05103581340520339\n",
      "ratio_wins2  -->  0.0510201829767451\n",
      "ratio_wins_opp1  -->  0.005755780034399949\n",
      "ratio_wins_opp2  -->  0.005783760512290157\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(cols)):\n",
    "    print(cols[i], ' --> ',model.feature_importances_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4150326797385621"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BernoulliNB()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.26      0.30        84\n",
      "           1       0.43      0.50      0.46       111\n",
      "           2       0.43      0.45      0.44       111\n",
      "\n",
      "   micro avg       0.42      0.42      0.42       306\n",
      "   macro avg       0.41      0.40      0.40       306\n",
      "weighted avg       0.41      0.42      0.41       306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\envs\\python\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3660130718954248"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ExtraTreesClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.23      0.25        84\n",
      "           1       0.40      0.50      0.44       111\n",
      "           2       0.38      0.34      0.36       111\n",
      "\n",
      "   micro avg       0.37      0.37      0.37       306\n",
      "   macro avg       0.35      0.35      0.35       306\n",
      "weighted avg       0.36      0.37      0.36       306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goals_past1  -->  0.07652979784407525\n",
      "goals_past2  -->  0.0769539399552229\n",
      "nb_draws1  -->  0.039996711148016145\n",
      "nb_draws2  -->  0.041931380884299235\n",
      "nb_draws_opp1  -->  0.005525180375878237\n",
      "nb_draws_opp2  -->  0.0053514082989847295\n",
      "nb_losses1  -->  0.03335287416244907\n",
      "nb_losses2  -->  0.032522324804748225\n",
      "nb_losses_opp1  -->  0.002719379586303315\n",
      "nb_losses_opp2  -->  0.003100486068667324\n",
      "nb_matches1  -->  0.05119860633377665\n",
      "nb_matches2  -->  0.052860492280565975\n",
      "nb_wins1  -->  0.029802027690442344\n",
      "nb_wins2  -->  0.03315469915646316\n",
      "nb_wins_opp1  -->  0.0033217818324466205\n",
      "nb_wins_opp2  -->  0.0034073623313006446\n",
      "ratio_draws1  -->  0.04766706370896587\n",
      "ratio_draws2  -->  0.04834620879378734\n",
      "ratio_draws_opp1  -->  0.004993513140241363\n",
      "ratio_draws_opp2  -->  0.005084548616493415\n",
      "ratio_goals_past1  -->  0.0887610631289277\n",
      "ratio_goals_past2  -->  0.09067553079207176\n",
      "ratio_losses1  -->  0.05285988960679507\n",
      "ratio_losses2  -->  0.05525532472458984\n",
      "ratio_losses_opp1  -->  0.0029125042827791974\n",
      "ratio_losses_opp2  -->  0.0028554517114022368\n",
      "ratio_wins1  -->  0.04952501057382923\n",
      "ratio_wins2  -->  0.05173163518415327\n",
      "ratio_wins_opp1  -->  0.004031723785421923\n",
      "ratio_wins_opp2  -->  0.0035720791969020074\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(cols)):\n",
    "    print(cols[i], ' --> ',model.feature_importances_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3464052287581699"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KNeighborsClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.26      0.24        84\n",
      "           1       0.41      0.44      0.43       111\n",
      "           2       0.41      0.32      0.36       111\n",
      "\n",
      "   micro avg       0.35      0.35      0.35       306\n",
      "   macro avg       0.35      0.34      0.34       306\n",
      "weighted avg       0.36      0.35      0.35       306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\envs\\python\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.43137254901960786"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLPClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.02      0.04        84\n",
      "           1       0.43      0.61      0.51       111\n",
      "           2       0.43      0.56      0.49       111\n",
      "\n",
      "   micro avg       0.43      0.43      0.43       306\n",
      "   macro avg       0.40      0.40      0.35       306\n",
      "weighted avg       0.41      0.43      0.37       306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39215686274509803"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NearestCentroid()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.36      0.32        84\n",
      "           1       0.45      0.41      0.42       111\n",
      "           2       0.45      0.41      0.42       111\n",
      "\n",
      "   micro avg       0.39      0.39      0.39       306\n",
      "   macro avg       0.39      0.39      0.39       306\n",
      "weighted avg       0.40      0.39      0.40       306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4117647058823529"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RidgeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        84\n",
      "           1       0.41      0.57      0.48       111\n",
      "           2       0.41      0.57      0.48       111\n",
      "\n",
      "   micro avg       0.41      0.41      0.41       306\n",
      "   macro avg       0.27      0.38      0.32       306\n",
      "weighted avg       0.30      0.41      0.35       306\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\envs\\python\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42483660130718953"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        84\n",
      "           1       0.42      0.59      0.49       111\n",
      "           2       0.42      0.59      0.49       111\n",
      "\n",
      "   micro avg       0.42      0.42      0.42       306\n",
      "   macro avg       0.28      0.39      0.33       306\n",
      "weighted avg       0.31      0.42      0.36       306\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\envs\\python\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

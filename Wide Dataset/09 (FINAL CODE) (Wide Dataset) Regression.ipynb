{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Wide_WithAreaDensity_V5.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dup = data[['Team2', 'Team1', 'Score2', 'Score1', 'Date/Time', 'nb_matches2', 'nb_matches1', 'goals_past2', 'goals_past1',\n",
    "                 'ratio_goals_past2', 'ratio_goals_past1', 'nb_wins2', 'nb_wins1', 'ratio_wins2', 'ratio_wins1', 'nb_losses2',\n",
    "                 'nb_losses1', 'ratio_losses2', 'ratio_losses1', 'nb_draws2', 'nb_draws1', 'ratio_draws2', 'ratio_draws1',\n",
    "                 'nb_wins_opp2', 'nb_wins_opp1', 'ratio_wins_opp2', 'ratio_wins_opp1', 'nb_losses_opp2', 'nb_losses_opp1',\n",
    "                 'ratio_losses_opp2', 'ratio_losses_opp1', 'nb_draws_opp2', 'nb_draws_opp1', 'ratio_draws_opp2',\n",
    "                 'ratio_draws_opp1', 'Ranks2', 'Ranks1', 'Points2', 'Points1', 'Year', 'Population2', 'Population1',\n",
    "                 'Surface2', 'Surface1', 'Density2', 'Density1']]\n",
    "data_dup.columns = ['Team1', 'Team2', 'Score1', 'Score2', 'Date/Time', 'nb_matches1', 'nb_matches2', 'goals_past1', \n",
    "                    'goals_past2', 'ratio_goals_past1', 'ratio_goals_past2', 'nb_wins1', 'nb_wins2', 'ratio_wins1',\n",
    "                    'ratio_wins2', 'nb_losses1', 'nb_losses2', 'ratio_losses1', 'ratio_losses2', 'nb_draws1', 'nb_draws2', \n",
    "                    'ratio_draws1', 'ratio_draws2', 'nb_wins_opp1', 'nb_wins_opp2', 'ratio_wins_opp1',\n",
    "                    'ratio_wins_opp2', 'nb_losses_opp1', 'nb_losses_opp2', 'ratio_losses_opp1', 'ratio_losses_opp2', \n",
    "                    'nb_draws_opp1', 'nb_draws_opp2', 'ratio_draws_opp1', 'ratio_draws_opp2', 'Ranks1', 'Ranks2', 'Points1',\n",
    "                    'Points2', 'Year', 'Population1', 'Population2', 'Surface1', 'Surface2', 'Density1', 'Density2']\n",
    "data = data.append(data_dup, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = data.loc[data[\"Date/Time\"] >= '2018']\n",
    "df_train = data.loc[data[\"Date/Time\"] < '2018']\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Target Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = []\n",
    "for i in range(len(df_train)):\n",
    "    if df_train['Score1'][i]+df_train['Score2'][i] == 0:\n",
    "        y_train.append(0)\n",
    "    else:\n",
    "        y_train.append(df_train['Score1'][i]/(df_train['Score1'][i]+df_train['Score2'][i]))\n",
    "        \n",
    "y_test = []\n",
    "for i in range(len(df_test)):\n",
    "    if df_test['Score1'][i] > df_test['Score2'][i]:\n",
    "        y_test.append(1)\n",
    "    elif df_test['Score1'][i] < df_test['Score2'][i]:\n",
    "        y_test.append(2)\n",
    "    elif df_test['Score1'][i] == df_test['Score2'][i]:\n",
    "        y_test.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Useless Columns: Getting X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(['Team1','Team2','Score1','Score2','Date/Time','Year'],axis=1)\n",
    "X_test = df_test.drop(['Team1','Team2','Score1','Score2','Date/Time','Year'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Team1', 'Team2', 'Score1', 'Score2', 'Date/Time', 'nb_matches1',\n",
       "       'nb_matches2', 'goals_past1', 'goals_past2', 'ratio_goals_past1',\n",
       "       'ratio_goals_past2', 'nb_wins1', 'nb_wins2', 'ratio_wins1',\n",
       "       'ratio_wins2', 'nb_losses1', 'nb_losses2', 'ratio_losses1',\n",
       "       'ratio_losses2', 'nb_draws1', 'nb_draws2', 'ratio_draws1',\n",
       "       'ratio_draws2', 'nb_wins_opp1', 'nb_wins_opp2', 'ratio_wins_opp1',\n",
       "       'ratio_wins_opp2', 'nb_losses_opp1', 'nb_losses_opp2',\n",
       "       'ratio_losses_opp1', 'ratio_losses_opp2', 'nb_draws_opp1',\n",
       "       'nb_draws_opp2', 'ratio_draws_opp1', 'ratio_draws_opp2', 'Ranks1',\n",
       "       'Ranks2', 'Points1', 'Points2', 'Year', 'Population1', 'Population2',\n",
       "       'Surface1', 'Surface2', 'Density1', 'Density2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-class Classification: Removing the observations corresponding to Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_to_class (y):\n",
    "    y_class = []\n",
    "    \n",
    "    for i in range(len(y)//2):\n",
    "        if abs(y[i] - y[i+len(y)//2]) < 0.03:\n",
    "            y_class.append(0)\n",
    "        elif y[i] > y[i+len(y)//2]:\n",
    "            y_class.append(1)\n",
    "        elif y[i] < y[i+len(y)//2]:\n",
    "            y_class.append(2)\n",
    "            \n",
    "    y_class2 = []\n",
    "    for i in y_class:\n",
    "        if i == 0:\n",
    "            y_class2.append(0)\n",
    "        elif i == 1:\n",
    "            y_class2.append(2)\n",
    "        elif i == 2:\n",
    "            y_class2.append(1)\n",
    "        \n",
    "    return(y_class+y_class2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_del = []\n",
    "for i in range(len(y_train)):\n",
    "    if reg_to_class(y_train)[i] == 0:\n",
    "        l_del.append(i)\n",
    "        \n",
    "X_train = X_train.drop(l_del, axis=0)\n",
    "\n",
    "y_train2 = []\n",
    "for i in range(len(y_train)):\n",
    "    if not(i in l_del):\n",
    "        y_train2.append(y_train[i])\n",
    "    \n",
    "X_train.reset_index(drop=True)\n",
    "y_train = y_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_del = []\n",
    "for i in range(len(y_test)):\n",
    "    if reg_to_class(y_test)[i] == 0:\n",
    "        l_del.append(i)\n",
    "        \n",
    "X_test = X_test.drop(l_del, axis=0)\n",
    "\n",
    "y_test2 = []\n",
    "for i in range(len(y_test)):\n",
    "    if not(i in l_del):\n",
    "        y_test2.append(y_test[i])\n",
    "    \n",
    "X_test.reset_index(drop=True)\n",
    "y_test = y_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\envs\\python\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\ASUS\\Anaconda3\\envs\\python\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\ASUS\\Anaconda3\\envs\\python\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you want to try the 2-class classification, please uncomment this cell\n",
    "\n",
    "# def reg_to_class (y):\n",
    "#     y_class = []\n",
    "    \n",
    "#     for i in range(len(y)//2):\n",
    "#         if y[i] >= y[i+len(y)//2]:\n",
    "#             y_class.append(1)\n",
    "#         elif y[i] < y[i+len(y)//2]:\n",
    "#             y_class.append(2)\n",
    "            \n",
    "#     y_class2 = []\n",
    "#     for i in y_class:\n",
    "#         if i == 1:\n",
    "#             y_class2.append(2)\n",
    "#         elif i == 2:\n",
    "#             y_class2.append(1)\n",
    "        \n",
    "#     return(y_class+y_class2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out Regression Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DummyRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "accuracy_score(reg_to_class(model.predict(X_test)),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00       503\n",
      "           2       0.00      0.00      0.00       503\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      1006\n",
      "   macro avg       0.00      0.00      0.00      1006\n",
      "weighted avg       0.00      0.00      0.00      1006\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\envs\\python\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\ASUS\\Anaconda3\\envs\\python\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, reg_to_class(model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.562624254473161"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLPRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "accuracy_score(reg_to_class(model.predict(X_test)),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.62      0.56      0.59       503\n",
      "           2       0.62      0.56      0.59       503\n",
      "\n",
      "   micro avg       0.56      0.56      0.56      1006\n",
      "   macro avg       0.41      0.38      0.39      1006\n",
      "weighted avg       0.62      0.56      0.59      1006\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\envs\\python\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, reg_to_class(model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6163021868787276"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "accuracy_score(reg_to_class(model.predict(X_test)),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.67      0.62      0.64       503\n",
      "           2       0.67      0.62      0.64       503\n",
      "\n",
      "   micro avg       0.62      0.62      0.62      1006\n",
      "   macro avg       0.45      0.41      0.43      1006\n",
      "weighted avg       0.67      0.62      0.64      1006\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\envs\\python\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, reg_to_class(model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\envs\\python\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5725646123260437"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "accuracy_score(reg_to_class(model.predict(X_test)),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.64      0.57      0.60       503\n",
      "           2       0.64      0.57      0.60       503\n",
      "\n",
      "   micro avg       0.57      0.57      0.57      1006\n",
      "   macro avg       0.43      0.38      0.40      1006\n",
      "weighted avg       0.64      0.57      0.60      1006\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\envs\\python\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, reg_to_class(model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_matches1  -->  0.021914959040027458\n",
      "nb_matches2  -->  0.024952730512294415\n",
      "goals_past1  -->  0.02702473170123937\n",
      "goals_past2  -->  0.028255470871859468\n",
      "ratio_goals_past1  -->  0.05003144338177383\n",
      "ratio_goals_past2  -->  0.056887490273537886\n",
      "nb_wins1  -->  0.007760844221412017\n",
      "nb_wins2  -->  0.007920731678405154\n",
      "ratio_wins1  -->  0.03075102188173474\n",
      "ratio_wins2  -->  0.029609859738588967\n",
      "nb_losses1  -->  0.008253885942685406\n",
      "nb_losses2  -->  0.007471461481307944\n",
      "ratio_losses1  -->  0.03240188322399161\n",
      "ratio_losses2  -->  0.032115803138667263\n",
      "nb_draws1  -->  0.012501462015065254\n",
      "nb_draws2  -->  0.0133389411274314\n",
      "ratio_draws1  -->  0.0316082649705399\n",
      "ratio_draws2  -->  0.02839804370587932\n",
      "nb_wins_opp1  -->  0.0020628551072736545\n",
      "nb_wins_opp2  -->  0.0018424523208705281\n",
      "ratio_wins_opp1  -->  0.002614061349884087\n",
      "ratio_wins_opp2  -->  0.0022335614916429818\n",
      "nb_losses_opp1  -->  0.001409279513063175\n",
      "nb_losses_opp2  -->  0.0021964319517308144\n",
      "ratio_losses_opp1  -->  0.0026719676118895355\n",
      "ratio_losses_opp2  -->  0.0025842773243121733\n",
      "nb_draws_opp1  -->  0.002455957069481822\n",
      "nb_draws_opp2  -->  0.0024149440580202132\n",
      "ratio_draws_opp1  -->  0.0035522661432920597\n",
      "ratio_draws_opp2  -->  0.002931738309953553\n",
      "Ranks1  -->  0.052961940510917196\n",
      "Ranks2  -->  0.04881510761020329\n",
      "Points1  -->  0.08146238499554077\n",
      "Points2  -->  0.08646846947834086\n",
      "Population1  -->  0.05446771957414363\n",
      "Population2  -->  0.052851670507018875\n",
      "Surface1  -->  0.02927602371940835\n",
      "Surface2  -->  0.028158888825539758\n",
      "Density1  -->  0.04046305863783205\n",
      "Density2  -->  0.044905914983199235\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(cols)):\n",
    "    print(cols[i], ' --> ',model.feature_importances_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.536779324055666"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AdaBoostRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "accuracy_score(reg_to_class(model.predict(X_test)),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.67      0.54      0.60       503\n",
      "           2       0.67      0.54      0.60       503\n",
      "\n",
      "   micro avg       0.54      0.54      0.54      1006\n",
      "   macro avg       0.45      0.36      0.40      1006\n",
      "weighted avg       0.67      0.54      0.60      1006\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\envs\\python\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, reg_to_class(model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5685884691848907"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BaggingRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "accuracy_score(reg_to_class(model.predict(X_test)),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.61      0.57      0.59       503\n",
      "           2       0.61      0.57      0.59       503\n",
      "\n",
      "   micro avg       0.57      0.57      0.57      1006\n",
      "   macro avg       0.41      0.38      0.39      1006\n",
      "weighted avg       0.61      0.57      0.59      1006\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\envs\\python\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, reg_to_class(model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6202783300198808"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformedTargetRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "accuracy_score(reg_to_class(model.predict(X_test)),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.68      0.62      0.65       503\n",
      "           2       0.68      0.62      0.65       503\n",
      "\n",
      "   micro avg       0.62      0.62      0.62      1006\n",
      "   macro avg       0.45      0.41      0.43      1006\n",
      "weighted avg       0.68      0.62      0.65      1006\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\Anaconda3\\envs\\python\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, reg_to_class(model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GradientBoostingRegressor(random_state=0)\n",
    "# model.fit(X_train, y_train)\n",
    "# T = [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.16,0.17,0.18,0.19,0.2]\n",
    "# R = []\n",
    "# for t in T:\n",
    "#     R.append(accuracy_score(reg_to_class(model.predict(X_test),t),y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(T,R)\n",
    "# plt.xlabel(\"Threshold\")\n",
    "# plt.ylabel(\"Accuracy of the Gradient Boosting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GradientBoostingRegressor(random_state=0)\n",
    "# model.fit(X_train, y_train)\n",
    "# T = [0.01,0.015,0.02,0.025,0.03,0.035,0.04]\n",
    "# R = []\n",
    "# for t in T:\n",
    "#     R.append(accuracy_score(reg_to_class(model.predict(X_test),t),y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(T,R)\n",
    "# plt.xlabel(\"Threshold\")\n",
    "# plt.ylabel(\"Accuracy of the Gradient Boosting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_test, reg_to_class(model.predict(X_test),0.03)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
